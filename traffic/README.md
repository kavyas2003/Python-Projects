# kavyas2003

  For this project, I experimented with various combinations and permutations of layers in the neural network. I initially started the process with one of each layer. I used Started with a Convolutional layer, Max pooling layer, then one layer to flatten, a dropout layer and a dense layer with ssoftmax activation. This yielded a very low percentage for accuracy. I first began by adding another dense layer right before the final layer. This layer, with relu activation, had a significant effect as the accuracy percentage increased significantly. Seeing as this was beneficial, I added another relu activated dense layer before the newly added on. This time instead of using 8, I used 16 instead. Once I again I saw an increase which helped my accuracy rate. I considered adding another dropout layer so that the network didn't get overly dependent on any specific node. I used the same value of .2. This again had a positive effect so I added another one following the next dense layer. This yielded a significantly lower accuracy rate so I removed it and began reconsidering dense layers. I added one more Dense layer prior to the second dropout layer and achieved the highest accuracy rate yet. This one had a value of 32, double than the one following it. I decided to try one more relu activated dense layer. I doubled the value once again and it had a very slight increase. I decided to leave it in place for now. I experimented with the dropout values finding that both lowering and increasing it did not have the intended effect. I checked to make sure with all the layers in place, no layer was counteracting the accuracy and when I removed an extra dense layer, I saw a slight but significant increase in the accuracy rate. After testing removing the other layers and seeing lower rates a decided the model I had was functioning well. 
    Overall, using trial and error I was able to find an optimal solution for my model. By testing adding and removing layers until the number no longer increased, I was able to find a relative maximum that ended up being fairly high. There were many factors to experiment with such as how many layers, what types, and what number to give them. After trying various options, I learned a lot and was able to create a well functioning model. 
